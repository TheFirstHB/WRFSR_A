{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9b7b97-ad19-4e90-a36d-e529c8cac97c",
   "metadata": {},
   "source": [
    "データ同化\n",
    "テストデータに対し同化後データに変換\n",
    "保存、検証に使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf3fa363-420d-44f0-b9f7-c1f15b74b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seedを固定\n",
    "#前提ライブラリを取得\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPVisionConfig, CLIPVisionModelWithProjection\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13e0c666-f52d-470d-8b57-2200dc0bc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#トレーニング設定\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "\n",
    "class TrainingConfig:\n",
    "\n",
    "    image_size = 64  # the generated image resolution\n",
    "    train_batch_size = 16\n",
    "    eval_batch_size = 16  # how many images to sample during evaluation\n",
    "\n",
    "    num_epochs = 10\n",
    "\n",
    "    num_timesteps = 1000\n",
    "    \n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "\n",
    "    save_model_epochs = 50\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "config = TrainingConfig()\n",
    "torch_fix_seed(config.seed)\n",
    "\n",
    "img_size = config.image_size\n",
    "batch = config.train_batch_size\n",
    "epochs = config.num_epochs\n",
    "lr_rate = config.learning_rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed1c27b-73f2-47d8-94d1-36d70336a556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DenoisingModel()\n",
    "save_path  = '/home/kurochan/sakuma/kenkiu/workspace_DLSR/DLSR-main/saved_model/'\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(save_path+'TMP_model1220.pth', map_location=\"cuda\",weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b30e687-d19a-44c6-acdc-9190019b3000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading三(　ﾟ∀ﾟ)...\n",
      "TMP_2021_manji.nc\n",
      "TMP_2022_manji.nc\n",
      "TMP_2023_manji.nc\n",
      "dataset to tensor(jstammt)...\n",
      "dataset making...\n",
      "kansei\n"
     ]
    }
   ],
   "source": [
    "def make_tensor(data='TMP',area='manji',years=[2021,2022,2023],source='/mnt/nadaru/trainingdataset'):\n",
    "    dss = []\n",
    "    for y in years:\n",
    "        filename = f\"{data}_{y}_{area}.nc\"\n",
    "        filepath = os.path.join(source,filename)\n",
    "        try:\n",
    "            d = xr.load_dataset(filepath)\n",
    "            dss.append(d)\n",
    "            print(filename)\n",
    "        except:\n",
    "            pass\n",
    "    ds = xr.concat(dss,dim=\"t\")\n",
    "    if np.sum(np.isnan(ds['amgsd'].values)) != 0:\n",
    "        print('Warning:nan detected')\n",
    "        ds.dropna(dim='y',how='any')\n",
    "    return ds\n",
    "\n",
    "#データセットをSSDから（これが時間かかる）\n",
    "print('dataset loading三(　ﾟ∀ﾟ)...')\n",
    "ds = make_tensor()\n",
    "\n",
    "lr = ds['WRF_1km'].to_numpy()\n",
    "hr = ds['WRF_300m'].values\n",
    "amgsd = ds['amgsd'].values\n",
    "timeline = ds['WRF_300m'].t\n",
    "timecode = np.arange(len(timeline))\n",
    "\n",
    "hr_mean= hr.mean()\n",
    "hr_std = hr.std()\n",
    "\n",
    "print('dataset to tensor(jstammt)...')\n",
    "lr_tensor = torch.tensor(lr.astype('float'),dtype=torch.float32)\n",
    "hr_tensor = torch.tensor(hr.astype('float'),dtype=torch.float32)\n",
    "timecode_tensor =  torch.tensor(timecode.astype('float'))\n",
    "amgsd_tensor = torch.tensor(amgsd.astype('float'))\n",
    "\n",
    "\n",
    "trans = torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.Resize(size=(64, 64)),\n",
    "                                        torchvision.transforms.Normalize((hr_mean), (hr_std))])\n",
    "\n",
    "class WRFdatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, LR, HR, amd, timeline, transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.lr = LR.unsqueeze(1)\n",
    "        self.hr = HR.unsqueeze(1)\n",
    "\n",
    "        self.time = timeline.to(torch.int)\n",
    "        self.amd  = amd.unsqueeze(1)\n",
    "        \n",
    "        self.datanum = len(timeline)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_lr = self.lr[idx]\n",
    "        out_hr = self.hr[idx]\n",
    "\n",
    "        out_amd = self.amd[idx]\n",
    "        out_time = self.time[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            out_lr = self.transform(out_lr)\n",
    "            out_hr = self.transform(out_hr)\n",
    "            out_amd = self.transform(out_amd)\n",
    "            \n",
    "\n",
    "        return out_lr, out_hr, out_amd, out_time\n",
    "\n",
    "print('dataset making...')\n",
    "dataset = WRFdatasets(lr_tensor, hr_tensor, amgsd_tensor, timecode_tensor, transform=trans)\n",
    "\n",
    "# 学習データ、検証データに 8:2 の割合で分割する。\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "print('kansei')\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch, shuffle = True, num_workers = 2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbc3a45-6ed7-417e-8934-e735b681f2e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'griddata' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m amd \u001b[38;5;241m=\u001b[39m amd\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m obs_num:\n\u001b[0;32m---> 28\u001b[0m     denoised_obs, array \u001b[38;5;241m=\u001b[39m diffuser\u001b[38;5;241m.\u001b[39msample_asim(model,low, amd, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m,asim_sample\u001b[38;5;241m=\u001b[39mobs)\n\u001b[1;32m     30\u001b[0m     result \u001b[38;5;241m=\u001b[39m denoised_obs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     31\u001b[0m     images \u001b[38;5;241m=\u001b[39m invTrans(result)\n",
      "Cell \u001b[0;32mIn[6], line 85\u001b[0m, in \u001b[0;36mDiffuser.sample_asim\u001b[0;34m(self, model, cond, obs, x_shape, gamma, asim_sample)\u001b[0m\n\u001b[1;32m     82\u001b[0m obs \u001b[38;5;241m=\u001b[39m obs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     84\u001b[0m obs_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrsampling(obs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mcopy(), asim_sample)\n\u001b[0;32m---> 85\u001b[0m interpolate \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmakeinterpolategrid(obs_points, size\u001b[38;5;241m=\u001b[39mx_shape[\u001b[38;5;241m2\u001b[39m:]),device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     86\u001b[0m mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositionmask(obs_points, size\u001b[38;5;241m=\u001b[39mx_shape[\u001b[38;5;241m2\u001b[39m:]),device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)): \u001b[38;5;66;03m#逆順イテレータ\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 133\u001b[0m, in \u001b[0;36mDiffuser.makeinterpolategrid\u001b[0;34m(self, obsarr, size)\u001b[0m\n\u001b[1;32m    130\u001b[0m data \u001b[38;5;241m=\u001b[39m [i[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m obsarr] \n\u001b[1;32m    131\u001b[0m latarr, longarr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmeshgrid(\u001b[38;5;28mrange\u001b[39m(size[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mrange\u001b[39m(size[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m--> 133\u001b[0m result1 \u001b[38;5;241m=\u001b[39m griddata(points\u001b[38;5;241m=\u001b[39mcoordarr, values\u001b[38;5;241m=\u001b[39mdata, xi\u001b[38;5;241m=\u001b[39m(latarr, longarr),method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcubic\u001b[39m\u001b[38;5;124m'\u001b[39m, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m#gaisou\u001b[39;00m\n\u001b[1;32m    135\u001b[0m result2 \u001b[38;5;241m=\u001b[39m griddata(points\u001b[38;5;241m=\u001b[39mcoordarr, values\u001b[38;5;241m=\u001b[39mdata, xi\u001b[38;5;241m=\u001b[39m(latarr, longarr),method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'griddata' is not defined"
     ]
    }
   ],
   "source": [
    "invTrans = transforms.Compose([ transforms.Normalize(mean = 0.,\n",
    "                                                     std = 1/hr_std),\n",
    "                                transforms.Normalize(mean = -hr_mean,\n",
    "                                                     std = 1),\n",
    "                               ])\n",
    "\n",
    "#assimilation\n",
    "'''\n",
    "テストデータ同化出力\n",
    "観測点数:300\n",
    "'''\n",
    "model.eval()\n",
    "diffuser = Diffuser(config.num_timesteps, device=device)\n",
    "\n",
    "obs_num = [300]\n",
    "denoised = {'denoised':[],'WRF_1km':[],'WRF_333m':[],'amgsd':[],'time':[]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for low, high, amd, time in testloader:\n",
    "        #条件付を削除\n",
    "        #low = torch.tensor(np.zeros(low.shape))\n",
    "        \n",
    "        low = low.to(device)\n",
    "        high = high.to(device)\n",
    "        amd = amd.to(device)\n",
    "\n",
    "        for obs in obs_num:\n",
    "            denoised_obs, array = diffuser.sample_asim(model,low, amd, gamma=1.0,asim_sample=obs)\n",
    "            \n",
    "            result = denoised_obs.to('cpu')\n",
    "            images = invTrans(result)\n",
    "            images = images.numpy()\n",
    "            \n",
    "            denoised['denoised'.format(obs)].append(images)\n",
    "            denoised['WRF_1km'].append(invTrans(low.to('cpu')).numpy())\n",
    "            denoised['WRF_333m'].append(invTrans(high.to('cpu')).numpy())\n",
    "            denoised['amgsd'].append(invTrans(amd.to('cpu')).numpy())\n",
    "            denoised['time'].append(time.to('cpu')).numpy()\n",
    "\n",
    "\n",
    "    denoised['denoised'] = np.concatenate(denoised['denoised'], 0).squeeze()\n",
    "    denoised['WRF_333m'] = np.concatenate(denoised['WRF_333m'], 0).squeeze()\n",
    "    denoised['WRF_1km'] = np.concatenate(denoised['WRF_1km'], 0).squeeze()\n",
    "    denoised['amgsd'] = np.concatenate(denoised['amgsd'], 0).squeeze()\n",
    "    denoised['time'] = np.concatenate(denoised['time'], 0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae73b9-18fb-4a15-b9de-fe18e93edf2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
