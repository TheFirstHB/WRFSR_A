{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3fa363-420d-44f0-b9f7-c1f15b74b6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#前提ライブラリ\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "\n",
    "#torch関連\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import CLIPVisionConfig\n",
    "\n",
    "#作成したモデルのインポート\n",
    "from models.Denoisingmodel import DenoisingModel\n",
    "from models.Diffusionmodel import Diffuser\n",
    "\n",
    "#その他\n",
    "import math\n",
    "import datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0c666-f52d-470d-8b57-2200dc0bc7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "条件設定\n",
    "シード固定\n",
    "'''\n",
    "\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "\n",
    "    image_size = 64 \n",
    "    train_batch_size = 32\n",
    "    \n",
    "\n",
    "    num_epochs = 20\n",
    "\n",
    "    num_timesteps = 1000\n",
    "    \n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "\n",
    "    save_model_epochs = 50\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "    clipconf = CLIPVisionConfig(\n",
    "        projection_dim = 512,\n",
    "        num_channels = 1,\n",
    "        image_size = 64\n",
    "        )\n",
    "config = TrainingConfig()\n",
    "torch_fix_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed1c27b-73f2-47d8-94d1-36d70336a556",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = config.image_size\n",
    "batch = config.train_batch_size\n",
    "epochs = config.num_epochs\n",
    "lr_rate = config.learning_rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = DenoisingModel(config)\n",
    "save_path  = '/home/kurochan/sakuma/kenkiu/workspace_DLSR/DLSR-main/saved_model/'\n",
    "\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(save_path+'TMP_model1220.pth', map_location=\"cuda\",weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30e687-d19a-44c6-acdc-9190019b3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tensor(data='TMP',area='manji',years=[2021,2022,2023],source='/mnt/nadaru/trainingdataset'):\n",
    "    dss = []\n",
    "    for y in years:\n",
    "        filename = f\"{data}_{y}_{area}.nc\"\n",
    "        filepath = os.path.join(source,filename)\n",
    "        try:\n",
    "            d = xr.load_dataset(filepath)\n",
    "            dss.append(d)\n",
    "            print(filename)\n",
    "        except:\n",
    "            pass\n",
    "    ds = xr.concat(dss,dim=\"t\")\n",
    "    if np.sum(np.isnan(ds['amgsd'].values)) != 0:\n",
    "        print('Warning:nan detected')\n",
    "        ds.dropna(dim='y',how='any')\n",
    "    return ds\n",
    "\n",
    "#データセットをSSDから（これが時間かかる）\n",
    "print('dataset loading三(　ﾟ∀ﾟ)...')\n",
    "ds = make_tensor()\n",
    "\n",
    "lr = ds['WRF_1km'].to_numpy()\n",
    "hr = ds['WRF_300m'].values\n",
    "amgsd = ds['amgsd'].values\n",
    "timeline = ds['WRF_300m'].t\n",
    "timecode = np.arange(len(timeline))\n",
    "\n",
    "hr_mean= hr.mean()\n",
    "hr_std = hr.std()\n",
    "\n",
    "print('dataset to tensor(jstammt)...')\n",
    "lr_tensor = torch.tensor(lr.astype('float'),dtype=torch.float32)\n",
    "hr_tensor = torch.tensor(hr.astype('float'),dtype=torch.float32)\n",
    "timecode_tensor =  torch.tensor(timecode.astype('float'))\n",
    "amgsd_tensor = torch.tensor(amgsd.astype('float'))\n",
    "\n",
    "\n",
    "trans = torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.Resize(size=(64, 64)),\n",
    "                                        torchvision.transforms.Normalize((hr_mean), (hr_std))])\n",
    "\n",
    "class WRFdatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, LR, HR, amd, timeline, transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.lr = LR.unsqueeze(1)\n",
    "        self.hr = HR.unsqueeze(1)\n",
    "\n",
    "        self.time = timeline.to(torch.int)\n",
    "        self.amd  = amd.unsqueeze(1)\n",
    "        \n",
    "        self.datanum = len(timeline)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_lr = self.lr[idx]\n",
    "        out_hr = self.hr[idx]\n",
    "\n",
    "        out_amd = self.amd[idx]\n",
    "        out_time = self.time[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            out_lr = self.transform(out_lr)\n",
    "            out_hr = self.transform(out_hr)\n",
    "            out_amd = self.transform(out_amd)\n",
    "            \n",
    "\n",
    "        return out_lr, out_hr, out_amd, out_time\n",
    "\n",
    "print('dataset making...')\n",
    "dataset = WRFdatasets(lr_tensor, hr_tensor, amgsd_tensor, timecode_tensor, transform=trans)\n",
    "\n",
    "# 学習データ、検証データに 8:2 の割合で分割する。\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "print('kansei')\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch, shuffle = True, num_workers = 2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbc3a45-6ed7-417e-8934-e735b681f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "invTrans = transforms.Compose([ transforms.Normalize(mean = 0.,\n",
    "                                                     std = 1/hr_std),\n",
    "                                transforms.Normalize(mean = -hr_mean,\n",
    "                                                     std = 1),\n",
    "                               ])\n",
    "\n",
    "#assimilation\n",
    "'''\n",
    "テストデータ同化出力\n",
    "観測点数:300\n",
    "'''\n",
    "model.eval()\n",
    "diffuser = Diffuser(config.num_timesteps, device=device)\n",
    "\n",
    "obs_num = [300]\n",
    "denoised = {'denoised':[],'WRF_1km':[],'WRF_333m':[],'amgsd':[],'time':[]}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for low, high, amd, time in testloader:\n",
    "        #条件付を削除\n",
    "        #low = torch.tensor(np.zeros(low.shape))\n",
    "        \n",
    "        low = low.to(device)\n",
    "        high = high.to(device)\n",
    "        amd = amd.to(device)\n",
    "\n",
    "        for obs in obs_num:\n",
    "            denoised_obs, array = diffuser.sample_asim(model, low, amd, gamma=1.0, asim_sample=obs)\n",
    "            \n",
    "            result = denoised_obs.to('cpu')\n",
    "            images = invTrans(result)\n",
    "            images = images.numpy()\n",
    "            \n",
    "            denoised['denoised'.format(obs)].append(images)\n",
    "            denoised['WRF_1km'].append(invTrans(low.to('cpu')).numpy())\n",
    "            denoised['WRF_333m'].append(invTrans(high.to('cpu')).numpy())\n",
    "            denoised['amgsd'].append(invTrans(amd.to('cpu')).numpy())\n",
    "            denoised['time'].append(time.to('cpu')).numpy()\n",
    "\n",
    "\n",
    "    denoised['denoised'] = np.concatenate(denoised['denoised'], 0).squeeze()\n",
    "    denoised['WRF_333m'] = np.concatenate(denoised['WRF_333m'], 0).squeeze()\n",
    "    denoised['WRF_1km'] = np.concatenate(denoised['WRF_1km'], 0).squeeze()\n",
    "    denoised['amgsd'] = np.concatenate(denoised['amgsd'], 0).squeeze()\n",
    "    denoised['time'] = np.concatenate(denoised['time'], 0)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
