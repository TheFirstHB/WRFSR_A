{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3d7a302-4d0a-426b-bdce-ca4da31fb2ce",
   "metadata": {},
   "source": [
    "diffusers.Unetモデルを流用したWRF学習\n",
    "トレーニングのみ行う\n",
    "\n",
    "1220\n",
    "エンコーダをモデルに統合（いずれ時系列も）\n",
    "TMP_model1220.pth ;\n",
    "\n",
    "1219\n",
    "前ブランチは統合済み\n",
    "低解像度入力を条件付に導入したい\n",
    "clipエンコーダによるベクトル化の導入...lossがあまり変わらない...\n",
    "DLR_model1219.pth ;lossがあまり変わらない...\n",
    "TMP_model1219.pth ;lossが4.0まで減少。やった！\n",
    "\n",
    "画像による条件付をベクトルで行った。10回実行したもの\n",
    "\n",
    "1218\n",
    "UNet2DConditionModelによる日付条件付きモデル（大きな改変が予測されるので別ブランチへ）\n",
    "日付をベクトルに変換し入力\n",
    "\n",
    "DLR_model1218...100回実行したもの。lossはあまり変わらなかった（9.2339）。\n",
    "わずかに収束が早くなった程度\n",
    "\n",
    "1010\n",
    "seedの確認と固定\n",
    "モデルのパラメータ保存を試みる\n",
    "条件付きモデルガイダンスを仕込んでみる\n",
    "画像生成と学習を別にしたい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb707508-4422-44ab-baf1-f117b4d8672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#seedを固定\n",
    "#前提ライブラリを取得\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from diffusers import UNet2DConditionModel\n",
    "from transformers import CLIPVisionConfig, CLIPVisionModelWithProjection\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179af08-96c1-4174-9197-0ad7c5581ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "条件設定\n",
    "シード固定\n",
    "'''\n",
    "\n",
    "\n",
    "def torch_fix_seed(seed=42):\n",
    "    # Python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Pytorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms = True\n",
    "\n",
    "@dataclass\n",
    "\n",
    "class TrainingConfig:\n",
    "\n",
    "    image_size = 64  # the generated image resolution\n",
    "    train_batch_size = 32\n",
    "    eval_batch_size = 32  # how many images to sample during evaluation\n",
    "\n",
    "    num_epochs = 20\n",
    "\n",
    "    num_timesteps = 1000\n",
    "    \n",
    "    gradient_accumulation_steps = 1\n",
    "    learning_rate = 1e-4\n",
    "    lr_warmup_steps = 500\n",
    "\n",
    "    save_model_epochs = 50\n",
    "\n",
    "    seed = 42\n",
    "\n",
    "config = TrainingConfig()\n",
    "torch_fix_seed(config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382ee014-8874-48cd-9296-b06714b1407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "年別データ結合\n",
    "'''\n",
    "\n",
    "def make_tensor(data='TMP',area='manji',years=[2021,2022,2023],source='/mnt/nadaru/trainingdataset'):\n",
    "    dss = []\n",
    "    for y in years:\n",
    "        filename = f\"{data}_{y}_{area}.nc\"\n",
    "        filepath = os.path.join(source,filename)\n",
    "        try:\n",
    "            d = xr.load_dataset(filepath)\n",
    "            dss.append(d)\n",
    "            print(filename)\n",
    "        except:\n",
    "            pass\n",
    "    ds = xr.concat(dss,dim=\"t\")\n",
    "    if np.sum(np.isnan(ds['amgsd'].values)) != 0:\n",
    "        print('Warning:nan detected')\n",
    "        ds.dropna(dim='y',how='any')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7641aa-ab6f-4d0a-80b1-77bc43015e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "ノイズ除去拡散モデル\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Diffuser:\n",
    "    def __init__(self, num_timesteps=1000, beta_start=0.0001, beta_end=0.02,device='cpu'):\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.device = device\n",
    "        self.betas = torch.linspace(beta_start, beta_end, num_timesteps, device=device)\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_bars = torch.cumprod(self.alphas, dim=0)\n",
    "        \n",
    "    def add_noise(self, x_0, t):\n",
    "        T = self.num_timesteps\n",
    "        assert (t >= 1).all() and ((t <= self.num_timesteps).all())\n",
    "        t_idx = t - 1\n",
    "        \n",
    "        alpha_bar = self.alpha_bars[t_idx]\n",
    "        N = alpha_bar.size(0)\n",
    "        alpha_bar = alpha_bar.view(N, 1, 1, 1)\n",
    "        \n",
    "        noise = torch.randn_like(x_0, device=self.device)\n",
    "        x_t = torch.sqrt(alpha_bar) * x_0 + torch.sqrt(1 - alpha_bar) * noise\n",
    "\n",
    "        return x_t, noise\n",
    "    \n",
    "    def denoise(self, model, x, cond, t, gamma):\n",
    "        T = self.num_timesteps\n",
    "        assert (t >= 1).all() and ((t <= self.num_timesteps).all())\n",
    "        \n",
    "        t_idx = t - 1\n",
    "        alpha = self.alphas[t_idx]\n",
    "        alpha_bar = self.alpha_bars[t_idx]\n",
    "        alpha_bar_prev = self.alpha_bars[t_idx - 1]\n",
    "        \n",
    "        N = alpha_bar.size(0)\n",
    "        alpha = alpha.view(N, 1, 1, 1)\n",
    "        alpha_bar = alpha_bar.view(N, 1, 1, 1)\n",
    "        alpha_bar_prev = alpha_bar_prev.view(N, 1, 1, 1)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                \n",
    "            input = torch.cat([x,cond],dim=1)\n",
    "            \n",
    "            #if torch.isnan(input).sum() != 0:\n",
    "            #    print('input!:')\n",
    "                \n",
    "            eps_cond = model(input, t).sample\n",
    "\n",
    "            if torch.isnan(eps_cond).sum() != 0:\n",
    "                print(torch.isnan(eps_cond).sum())\n",
    "            \n",
    "            nocond = torch.zeros_like(x, device=self.device)\n",
    "            input_uncond = torch.cat([x,nocond],dim=1)\n",
    "                \n",
    "            eps_uncond = model(input, t).sample\n",
    "            \n",
    "            eps = eps_uncond + gamma * (eps_cond - eps_uncond)\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        noise = torch.randn_like(x, device=self.device)\n",
    "        noise[t == 1] = 0\n",
    "\n",
    "        mu = (x - ((1-alpha) / torch.sqrt(1-alpha_bar)) * eps ) / torch.sqrt(alpha)\n",
    "    \n",
    "        std = torch.sqrt((1-alpha) * (1-alpha_bar_prev) / (1-alpha_bar))\n",
    "\n",
    "        return mu + noise * std\n",
    "    \n",
    "    def reverse_to_data(self, x):\n",
    "        #標準化の解除、numpy化\n",
    "        invTrans = transforms.Compose([ transforms.Normalize(mean = 0., std = 1/hr_std),\n",
    "                                        transforms.Normalize(mean = -hr_mean, std = 1), ])\n",
    "        \n",
    "        image = x.to('cpu')\n",
    "        image = invTrans(image)\n",
    "        image = image.numpy()\n",
    "\n",
    "        return np.transpose(image,(1,2,0))\n",
    "        \n",
    "    def sample(self, model, cond, x_shape=(16, 1, 64, 64),gamma=3.0):\n",
    "        #ノイズ除去、データ変換まで\n",
    "        batch_size = x_shape[0]\n",
    "        x = torch.randn(x_shape, device=self.device)\n",
    "        \n",
    "        for i in tqdm(range(self.num_timesteps, 0, -1)): \n",
    "            t = torch.tensor([i] * batch_size, device=self.device, dtype=torch.long)\n",
    "            x = self.denoise(model, x, cond, t, gamma)\n",
    "            #print(x.shape)\n",
    "        images = [self.reverse_to_data(x[i]) for i in range(batch_size)]\n",
    "        return x, images\n",
    "\n",
    "    def sample_asim(self, model, cond, obs, x_shape=(16, 1, 64, 64),gamma=3.0, asim_sample=100):\n",
    "        #データ同化処理\n",
    "        batch_size = x_shape[0]\n",
    "        x = torch.randn(x_shape, device=self.device)\n",
    "        obs = obs.to('cpu')\n",
    "        \n",
    "        obs_points = self.rsampling(obs.detach().numpy().copy(), asim_sample)\n",
    "        interpolate = torch.tensor(self.makeinterpolategrid(obs_points, size=x_shape[2:]),device=self.device)\n",
    "        mask = torch.tensor(self.positionmask(obs_points, size=x_shape[2:]),device=self.device)\n",
    "        \n",
    "        for i in tqdm(range(self.num_timesteps, 0, -1)): #逆順イテレータ\n",
    "            t = torch.tensor([i] * batch_size, device=self.device, dtype=torch.long)    \n",
    "            x = self.denoise(model, x, cond, t, gamma)\n",
    "            \n",
    "            noised_interp, noise = self.add_noise(interpolate, t)\n",
    "            \n",
    "            xunknown = (1-mask) * x\n",
    "            xknown = interpolate * mask\n",
    "\n",
    "\n",
    "                \n",
    "            x = (xunknown + xknown).to(torch.float32)\n",
    "            #print(x.shape)\n",
    "        \n",
    "        images = [self.reverse_to_data(x[i]) for i in range(batch_size)]\n",
    "        return x, images\n",
    "\n",
    "    def rsampling(self, darray, num_sample):\n",
    "        x_size, y_size = darray.shape[2:]\n",
    "        a = np.arange(x_size*y_size)\n",
    "        np.random.shuffle(a)\n",
    "        point = a[:num_sample]\n",
    "        sample_array = []\n",
    "        for i in point:\n",
    "            y_idx = i // x_size\n",
    "            x_idx = i % x_size\n",
    "            sample = y_idx, x_idx, darray[:,:, x_idx, y_idx]\n",
    "            sample_array.append(sample)\n",
    "        return sample_array\n",
    "\n",
    "    def fixedsampling(self, darray,coordarr):\n",
    "        sample_array = []\n",
    "        for i in coordarr:\n",
    "            y_idx = i[0]\n",
    "            x_idx = i[1]\n",
    "            sample = y_idx, x_idx, darray[:,:, x_idx, y_idx]\n",
    "            sample_array.append(sample)\n",
    "        return sample_array\n",
    "\n",
    "    #観測値補間\n",
    "    def makeinterpolategrid(self, obsarr, size=(64,64)):\n",
    "        coordarr = [i[0:2] for i in obsarr] \n",
    "        data = [i[2] for i in obsarr] \n",
    "        latarr, longarr = np.meshgrid(range(size[0]),range(size[1]))\n",
    "        \n",
    "        result1 = griddata(points=coordarr, values=data, xi=(latarr, longarr),method='cubic', fill_value=0).transpose(2,3,0,1)\n",
    "        #gaisou\n",
    "        result2 = griddata(points=coordarr, values=data, xi=(latarr, longarr),method='nearest').transpose(2,3,0,1)\n",
    "    \n",
    "        nan_mask = result1[0] == 0\n",
    "        result = result2 * nan_mask + result1\n",
    "        return result\n",
    "    \n",
    "    #ガウシアンマスクの作成\n",
    "    def _positionmask(self, coord, maskarr, kernel):\n",
    "        temp = np.zeros(maskarr.shape)\n",
    "        \n",
    "        latidx = coord[0] \n",
    "        lonidx = coord[1]\n",
    "        \n",
    "        temp[lonidx,latidx] = 1\n",
    "        temp = cv2.filter2D(temp,-1,kernel,borderType=cv2.BORDER_ISOLATED)\n",
    "        maskarr = np.fmax(temp,maskarr)\n",
    "        return maskarr\n",
    "    \n",
    "    def positionmask(self, obsarr, size=(64,64)):\n",
    "        #ガウシアンカーネルの用意\n",
    "        kernelg=cv2.getGaussianKernel(11,2.)\n",
    "        gaussian_2d = kernelg * kernelg.T\n",
    "        \n",
    "        maskarr = np.zeros(size)\n",
    "        coordarr = [i[0:2] for i in obsarr]\n",
    "        for coord in coordarr:\n",
    "            maskarr = self._positionmask(coord, maskarr, gaussian_2d)\n",
    "    \n",
    "        return maskarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512df63-4618-407a-9678-bbc99eb8210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loading三(　ﾟ∀ﾟ)...\n",
      "TMP_2021_manji.nc\n",
      "TMP_2022_manji.nc\n",
      "TMP_2023_manji.nc\n",
      "dataset to tensor(jstammt)...\n",
      "dataset making...\n",
      "kansei\n"
     ]
    }
   ],
   "source": [
    "#データセット取得：from WRFlearning.ipynb\n",
    "#改造：amgsdと時系列を取り出せるようにした\n",
    "\n",
    "img_size = config.image_size\n",
    "batch = config.train_batch_size\n",
    "epochs = config.num_epochs\n",
    "lr_rate = config.learning_rate\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#データセットの取得\n",
    "print('dataset loading三( ﾟ∀ﾟ)...')\n",
    "ds = make_tensor()\n",
    "\n",
    "lr = ds['WRF_1km'].values\n",
    "hr = ds['WRF_300m'].values\n",
    "amgsd = ds['amgsd'].values\n",
    "timeline = ds['WRF_300m'].t\n",
    "timecode = np.arange(len(timeline))\n",
    "\n",
    "hr_mean= hr.mean()\n",
    "hr_std = hr.std()\n",
    "\n",
    "print('dataset to tensor(jstammt)...')\n",
    "lr_tensor = torch.tensor(lr.astype('float'),dtype=torch.float32)\n",
    "hr_tensor = torch.tensor(hr.astype('float'),dtype=torch.float32)\n",
    "timecode_tensor =  torch.tensor(timecode.astype('float'))\n",
    "amgsd_tensor = torch.tensor(amgsd.astype('float'))\n",
    "\n",
    "\n",
    "trans = torchvision.transforms.Compose([\n",
    "                                        torchvision.transforms.Resize(size=(64, 64)),\n",
    "                                        torchvision.transforms.Normalize((hr_mean), (hr_std))])\n",
    "\n",
    "class WRFdatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, LR, HR, amd, timeline, transform = None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.lr = LR.unsqueeze(1)\n",
    "        self.hr = HR.unsqueeze(1)\n",
    "\n",
    "        self.time = timeline.to(torch.int)\n",
    "        self.amd  = amd.unsqueeze(1)\n",
    "        \n",
    "        self.datanum = len(timeline)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out_lr = self.lr[idx]\n",
    "        out_hr = self.hr[idx]\n",
    "\n",
    "        out_amd = self.amd[idx]\n",
    "        out_time = self.time[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            out_lr = self.transform(out_lr)\n",
    "            out_hr = self.transform(out_hr)\n",
    "            out_amd = self.transform(out_amd)\n",
    "            \n",
    "\n",
    "        return out_lr, out_hr, out_amd, out_time\n",
    "\n",
    "print('dataset making...')\n",
    "dataset = WRFdatasets(lr_tensor, hr_tensor, amgsd_tensor, timecode_tensor, transform=trans)\n",
    "\n",
    "# 学習データ、検証データに 8:2 の割合で分割する。\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(\n",
    "    dataset, [train_size, val_size]\n",
    ")\n",
    "print('kansei')\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size = batch, shuffle = True, num_workers = 2)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size = batch, shuffle = False, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "77d6e1a4-815e-4396-84db-407887528bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:12.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:3.3291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.7957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.5462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.4456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.3388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.3002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.0621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.0487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:27<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.2027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.1049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:27<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.0911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.9406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:2.0030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.9918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 657/657 [02:28<00:00,  4.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:1.9286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#モデル形成\n",
    "model = DenoisingModel()\n",
    "\n",
    "num_timesteps = config.num_timesteps\n",
    "diffuser = Diffuser(config.num_timesteps, device=device)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n",
    "\n",
    "#学習\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    loss_sum = 0.0\n",
    "    cnt = 0\n",
    "    \n",
    "    for low, high, amd, time in tqdm(trainloader):\n",
    "        \n",
    "        #時系列エンコード\n",
    "        #dates = pd.to_datetime(timeline[time].values)\n",
    "        #dates_encoded = datetime_embedder(dates).unsqueeze(1).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        low = low.to(device)\n",
    "        high = high.to(device)\n",
    "        t = torch.randint(1, config.num_timesteps+1, (len(high),), device=device)\n",
    "\n",
    "        x_noisy, noise = diffuser.add_noise(high,t) #画像にノイズ付加\n",
    "        noise_pred = model(t, x_noisy, low)\n",
    "        loss = F.mse_loss(noise, noise_pred)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        cnt += 1\n",
    "    print('loss:{:.4f}'.format(loss_sum))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef7c82dc-03b0-4514-b326-89d7de655cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#学習後パラメータ保存\n",
    "torch.save(model.state_dict(), 'saved_model/TMP_model1220.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c70388-9e97-4315-945c-b9d89d496068",
   "metadata": {},
   "source": [
    "rmse(K):2.459355308909517"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2368a-655d-438f-9f0e-572d1e869906",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
